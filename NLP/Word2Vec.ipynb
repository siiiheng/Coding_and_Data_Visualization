{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This dataset is intentionally a subset of all of the US iTunes podcasts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- prepr\n",
    "- postive 4,5\n",
    "- negative 1,2\n",
    "- -\n",
    "- regex\n",
    "- -\n",
    "- word2vec\n",
    "- topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Êú¨‰æÜÂ∞±ÊòØÈáùÂ∞çgenreÂéªÂÅö ÊØèÂÄãËÅΩÁúæÂ∞çÊñº‰∏çÂêågenreÁöÑÁúãÊ≥ï‰∏ç‰∏ÄÔºàÈô§ÈùûÂÜçÈáùÂ∞çÊüêÂÄãgenreÂéªÂÅö)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Package & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "import en_core_web_md\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sqlite3\n",
    "\n",
    "# Read sqlite query results into a pandas DataFrame\n",
    "# con = sqlite3.connect(\"database.sqlite\")\n",
    "# pod = pd.read_sql_query(\"SELECT * from podcasts\", con)\n",
    "# reviews = pd.read_sql_query(\"SELECT * from reviews\", con)\n",
    "# cat = pd.read_sql_query(\"SELECT * from categories\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('podcast_sample.csv', lineterminator='\\n', index_col = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>podcast_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>rating</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>itunes_id</th>\n",
       "      <th>slug</th>\n",
       "      <th>itunes_url</th>\n",
       "      <th>podcast_title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b313ef8ef0d5b64290d3036ff1bbf2d2</td>\n",
       "      <td>Í∞êÏÑ± ÎùºÎîîÏò§ ÏùåÏïÖÎèÑÏãú</td>\n",
       "      <td>ÎØ∏Íµ≠ ÏÑúÎ∂ÄÏóê ÏûàÎäî Ïú†ÌïôÏÉùÏù¥ÏóêÏöî. ÏÑ±ÏãúÍ≤ΩÏî® Ï†úÎåÄ ÌõÑ ÎùºÎîîÏò§ Î≥µÍ∑ÄÎßå Í∏∞Îã§Î†§Ïò§Îã§Í∞Ä 6 Ïõî...</td>\n",
       "      <td>5</td>\n",
       "      <td>664CCA7142E9AE8</td>\n",
       "      <td>2011-09-14T13:25:46-07:00</td>\n",
       "      <td>442838670</td>\n",
       "      <td>fm-%EC%9D%8C%EC%95%85%EB%8F%84%EC%8B%9C-%EC%A2...</td>\n",
       "      <td>https://podcasts.apple.com/us/podcast/fm-%EC%9...</td>\n",
       "      <td>FM ÏùåÏïÖÎèÑÏãú(Ï¢ÖÏòÅ)</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abfb842993be20d21bfae7103addc5e9</td>\n",
       "      <td>They‚Äôve really cut back on the content this se...</td>\n",
       "      <td>Last season there was a new pod every 3-4 days...</td>\n",
       "      <td>1</td>\n",
       "      <td>AD790CE113DCBC1</td>\n",
       "      <td>2018-04-11T13:46:47-07:00</td>\n",
       "      <td>1015394113</td>\n",
       "      <td>the-good-phight-for-philadelphia-phillies-fans</td>\n",
       "      <td>https://podcasts.apple.com/us/podcast/the-good...</td>\n",
       "      <td>The Good Phight: for Philadelphia Phillies fans</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ebdf879a424547d01862a9bbba18a0f3</td>\n",
       "      <td>Good info. source...</td>\n",
       "      <td>Bob brings a lot of knowledge to any firearm d...</td>\n",
       "      <td>4</td>\n",
       "      <td>E223A4B2642C970</td>\n",
       "      <td>2010-01-19T08:11:43-07:00</td>\n",
       "      <td>333180229</td>\n",
       "      <td>handgun-world-podcast</td>\n",
       "      <td>https://podcasts.apple.com/us/podcast/handgun-...</td>\n",
       "      <td>Handgun World Podcast</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ab2fdb7db023b223d870487165d11ff3</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>They have lost much of thier credibility by de...</td>\n",
       "      <td>3</td>\n",
       "      <td>E1E7DBE750D119E</td>\n",
       "      <td>2021-01-28T12:21:49-07:00</td>\n",
       "      <td>971901464</td>\n",
       "      <td>wsj-opinion-potomac-watch</td>\n",
       "      <td>https://podcasts.apple.com/us/podcast/wsj-opin...</td>\n",
       "      <td>WSJ Opinion: Potomac Watch</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ca601bd1524322d0527b16adf2738ff3</td>\n",
       "      <td>Try it now!</td>\n",
       "      <td>Even better than I expected. I was interested ...</td>\n",
       "      <td>5</td>\n",
       "      <td>D7CA4858AFA2CFC</td>\n",
       "      <td>2017-08-24T10:55:20-07:00</td>\n",
       "      <td>1257821731</td>\n",
       "      <td>conversations-with-people-who-hate-me</td>\n",
       "      <td>https://podcasts.apple.com/us/podcast/conversa...</td>\n",
       "      <td>Conversations with People Who Hate Me</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         podcast_id  \\\n",
       "0  b313ef8ef0d5b64290d3036ff1bbf2d2   \n",
       "1  abfb842993be20d21bfae7103addc5e9   \n",
       "2  ebdf879a424547d01862a9bbba18a0f3   \n",
       "3  ab2fdb7db023b223d870487165d11ff3   \n",
       "4  ca601bd1524322d0527b16adf2738ff3   \n",
       "\n",
       "                                               title  \\\n",
       "0                                        Í∞êÏÑ± ÎùºÎîîÏò§ ÏùåÏïÖÎèÑÏãú   \n",
       "1  They‚Äôve really cut back on the content this se...   \n",
       "2                               Good info. source...   \n",
       "3                                              Mixed   \n",
       "4                                        Try it now!   \n",
       "\n",
       "                                             content  rating        author_id  \\\n",
       "0  ÎØ∏Íµ≠ ÏÑúÎ∂ÄÏóê ÏûàÎäî Ïú†ÌïôÏÉùÏù¥ÏóêÏöî. ÏÑ±ÏãúÍ≤ΩÏî® Ï†úÎåÄ ÌõÑ ÎùºÎîîÏò§ Î≥µÍ∑ÄÎßå Í∏∞Îã§Î†§Ïò§Îã§Í∞Ä 6 Ïõî...       5  664CCA7142E9AE8   \n",
       "1  Last season there was a new pod every 3-4 days...       1  AD790CE113DCBC1   \n",
       "2  Bob brings a lot of knowledge to any firearm d...       4  E223A4B2642C970   \n",
       "3  They have lost much of thier credibility by de...       3  E1E7DBE750D119E   \n",
       "4  Even better than I expected. I was interested ...       5  D7CA4858AFA2CFC   \n",
       "\n",
       "                  created_at   itunes_id  \\\n",
       "0  2011-09-14T13:25:46-07:00   442838670   \n",
       "1  2018-04-11T13:46:47-07:00  1015394113   \n",
       "2  2010-01-19T08:11:43-07:00   333180229   \n",
       "3  2021-01-28T12:21:49-07:00   971901464   \n",
       "4  2017-08-24T10:55:20-07:00  1257821731   \n",
       "\n",
       "                                                slug  \\\n",
       "0  fm-%EC%9D%8C%EC%95%85%EB%8F%84%EC%8B%9C-%EC%A2...   \n",
       "1     the-good-phight-for-philadelphia-phillies-fans   \n",
       "2                              handgun-world-podcast   \n",
       "3                          wsj-opinion-potomac-watch   \n",
       "4              conversations-with-people-who-hate-me   \n",
       "\n",
       "                                          itunes_url  \\\n",
       "0  https://podcasts.apple.com/us/podcast/fm-%EC%9...   \n",
       "1  https://podcasts.apple.com/us/podcast/the-good...   \n",
       "2  https://podcasts.apple.com/us/podcast/handgun-...   \n",
       "3  https://podcasts.apple.com/us/podcast/wsj-opin...   \n",
       "4  https://podcasts.apple.com/us/podcast/conversa...   \n",
       "\n",
       "                                     podcast_title category  \n",
       "0                                      FM ÏùåÏïÖÎèÑÏãú(Ï¢ÖÏòÅ)    music  \n",
       "1  The Good Phight: for Philadelphia Phillies fans   sports  \n",
       "2                            Handgun World Podcast     news  \n",
       "3                       WSJ Opinion: Potomac Watch     news  \n",
       "4            Conversations with People Who Hate Me  society  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = df[df['rating']>= 4]\n",
    "bad_reviews = df[df['rating']<=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pt = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Justin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Justin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # A popular NLTK sentence tokenizer\n",
    "nltk.download('stopwords') # library of common English stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_clean(review):\n",
    "    review = review.strip().lower()\n",
    "    review = re.sub(r'\\'ve|\\'s|\\'m|\\'re|\\d', '', review)\n",
    "    review = re.sub(r'\\b(pod|podcast|great|really|good|listen|episode|highly)+(s?|es?)\\b','', review)\n",
    "    review = re.sub(r'\\b(listen|listens|listening|listened|forward|look forward|looking forward|loos forward)\\b', '', review)\n",
    "    review = re.sub(r'\\b(love|like|best)+(s?|es?)\\b','', review)\n",
    "\n",
    "    \n",
    "    word_tokens = word_tokenize(review)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    word_tokens = filtered_sentence\n",
    "#     word_tokens = [pt.stem(w) for w in word_tokens]\n",
    "    word_tokens = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    text_clean = ' '.join(word_tokens)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-7cb41d7f3e84>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_reviews['content'] = good_reviews.apply(lambda x: review_clean(x['content']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "good_reviews['content'] = good_reviews.apply(lambda x: review_clean(x['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 2984),\n",
       " ('make', 2659),\n",
       " ('work', 2603),\n",
       " ('guy', 2499),\n",
       " ('story', 2449),\n",
       " ('way', 2306),\n",
       " ('thank', 2203),\n",
       " ('life', 2118),\n",
       " ('new', 2074),\n",
       " ('know', 1971),\n",
       " ('guest', 1970),\n",
       " ('people', 1933),\n",
       " ('thing', 1908),\n",
       " ('feel', 1707),\n",
       " ('hear', 1703),\n",
       " ('host', 1697),\n",
       " ('topic', 1561),\n",
       " ('fun', 1527),\n",
       " ('year', 1518),\n",
       " ('day', 1471),\n",
       " ('content', 1451),\n",
       " ('lot', 1432),\n",
       " ('enjoy', 1393),\n",
       " ('think', 1386),\n",
       " ('need', 1382),\n",
       " ('want', 1377),\n",
       " ('thanks', 1376),\n",
       " ('interesting', 1367),\n",
       " ('week', 1362),\n",
       " ('amazing', 1345),\n",
       " ('talk', 1317),\n",
       " ('favorite', 1284),\n",
       " ('funny', 1254),\n",
       " ('better', 1207),\n",
       " ('real', 1201),\n",
       " ('say', 1194),\n",
       " ('friend', 1171),\n",
       " ('world', 1126),\n",
       " ('information', 1094),\n",
       " ('fan', 1092),\n",
       " ('help', 1075),\n",
       " ('come', 1052),\n",
       " ('job', 1048),\n",
       " ('wait', 1035),\n",
       " ('recommend', 1015),\n",
       " ('going', 1014),\n",
       " ('voice', 1001),\n",
       " ('conversation', 979),\n",
       " ('interview', 976),\n",
       " ('entertaining', 933)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(stop_words=\"english\", binary=True).fit(good_reviews[\"content\"])\n",
    "bag_of_words = vec.transform(good_reviews[\"content\"])\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "words_freq[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=0.4, max_features=200, stop_words=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(good_reviews[\"content\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Justin/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aldiz', 'baiknya', 'baizik', 'berkali', 'bukatzeko', 'edota', 'eze', 'ezpabere', 'ezpada', 'ezperen', 'gainera', 'gainerontzean', 'guztiz', 'hainbestez', 'horra', 'kali', 'kurangnya', 'mata', 'olah', 'ordea', 'osterantzean', 'printr', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/Justin/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advice information helpful</th>\n",
       "      <th>amazing amazing amazing</th>\n",
       "      <th>amazing story teller</th>\n",
       "      <th>appreciate hard work</th>\n",
       "      <th>appreciate time effort</th>\n",
       "      <th>asks question guest</th>\n",
       "      <th>awesome work guy</th>\n",
       "      <th>banter make laugh</th>\n",
       "      <th>black life matter</th>\n",
       "      <th>blah blah blah</th>\n",
       "      <th>...</th>\n",
       "      <th>waiting long time</th>\n",
       "      <th>watching youtube video</th>\n",
       "      <th>wide range topic</th>\n",
       "      <th>wide variety guest</th>\n",
       "      <th>wide variety topic</th>\n",
       "      <th>wish found sooner</th>\n",
       "      <th>wish give star</th>\n",
       "      <th>work day faster</th>\n",
       "      <th>wow world wow</th>\n",
       "      <th>wow wow wow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28244</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28245</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28246</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28247</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28248</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28249 rows √ó 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       advice information helpful  amazing amazing amazing  \\\n",
       "0                             0.0                      0.0   \n",
       "1                             0.0                      0.0   \n",
       "2                             0.0                      0.0   \n",
       "3                             0.0                      0.0   \n",
       "4                             0.0                      0.0   \n",
       "...                           ...                      ...   \n",
       "28244                         0.0                      0.0   \n",
       "28245                         0.0                      0.0   \n",
       "28246                         0.0                      0.0   \n",
       "28247                         0.0                      0.0   \n",
       "28248                         0.0                      0.0   \n",
       "\n",
       "       amazing story teller  appreciate hard work  appreciate time effort  \\\n",
       "0                       0.0                   0.0                     0.0   \n",
       "1                       0.0                   0.0                     0.0   \n",
       "2                       0.0                   0.0                     0.0   \n",
       "3                       0.0                   0.0                     0.0   \n",
       "4                       0.0                   0.0                     0.0   \n",
       "...                     ...                   ...                     ...   \n",
       "28244                   0.0                   0.0                     0.0   \n",
       "28245                   0.0                   0.0                     0.0   \n",
       "28246                   0.0                   0.0                     0.0   \n",
       "28247                   0.0                   0.0                     0.0   \n",
       "28248                   0.0                   0.0                     0.0   \n",
       "\n",
       "       asks question guest  awesome work guy  banter make laugh  \\\n",
       "0                      0.0               0.0                0.0   \n",
       "1                      0.0               0.0                0.0   \n",
       "2                      0.0               0.0                0.0   \n",
       "3                      0.0               0.0                0.0   \n",
       "4                      0.0               0.0                0.0   \n",
       "...                    ...               ...                ...   \n",
       "28244                  0.0               0.0                0.0   \n",
       "28245                  0.0               0.0                0.0   \n",
       "28246                  0.0               0.0                0.0   \n",
       "28247                  0.0               0.0                0.0   \n",
       "28248                  0.0               0.0                0.0   \n",
       "\n",
       "       black life matter  blah blah blah  ...  waiting long time  \\\n",
       "0                    0.0             0.0  ...                0.0   \n",
       "1                    0.0             0.0  ...                0.0   \n",
       "2                    0.0             0.0  ...                0.0   \n",
       "3                    0.0             0.0  ...                0.0   \n",
       "4                    0.0             0.0  ...                0.0   \n",
       "...                  ...             ...  ...                ...   \n",
       "28244                0.0             0.0  ...                0.0   \n",
       "28245                0.0             0.0  ...                0.0   \n",
       "28246                0.0             0.0  ...                0.0   \n",
       "28247                0.0             0.0  ...                0.0   \n",
       "28248                0.0             0.0  ...                0.0   \n",
       "\n",
       "       watching youtube video  wide range topic  wide variety guest  \\\n",
       "0                         0.0               0.0                 0.0   \n",
       "1                         0.0               0.0                 0.0   \n",
       "2                         0.0               0.0                 0.0   \n",
       "3                         0.0               0.0                 0.0   \n",
       "4                         0.0               0.0                 0.0   \n",
       "...                       ...               ...                 ...   \n",
       "28244                     0.0               0.0                 0.0   \n",
       "28245                     0.0               0.0                 0.0   \n",
       "28246                     0.0               0.0                 0.0   \n",
       "28247                     0.0               0.0                 0.0   \n",
       "28248                     0.0               0.0                 0.0   \n",
       "\n",
       "       wide variety topic  wish found sooner  wish give star  work day faster  \\\n",
       "0                     0.0                0.0             0.0              0.0   \n",
       "1                     0.0                0.0             0.0              0.0   \n",
       "2                     0.0                0.0             0.0              0.0   \n",
       "3                     0.0                0.0             0.0              0.0   \n",
       "4                     0.0                0.0             0.0              0.0   \n",
       "...                   ...                ...             ...              ...   \n",
       "28244                 0.0                0.0             0.0              0.0   \n",
       "28245                 0.0                0.0             0.0              0.0   \n",
       "28246                 0.0                0.0             0.0              0.0   \n",
       "28247                 0.0                0.0             0.0              0.0   \n",
       "28248                 0.0                0.0             0.0              0.0   \n",
       "\n",
       "       wow world wow  wow wow wow  \n",
       "0                0.0          0.0  \n",
       "1                0.0          0.0  \n",
       "2                0.0          0.0  \n",
       "3                0.0          0.0  \n",
       "4                0.0          0.0  \n",
       "...              ...          ...  \n",
       "28244            0.0          0.0  \n",
       "28245            0.0          0.0  \n",
       "28246            0.0          0.0  \n",
       "28247            0.0          0.0  \n",
       "28248            0.0          0.0  \n",
       "\n",
       "[28249 rows x 200 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breath fresh air</th>\n",
       "      <td>59.836317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long time listener</th>\n",
       "      <td>36.313809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make laugh loud</th>\n",
       "      <td>28.050138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel hanging friend</th>\n",
       "      <td>24.250959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real estate investing</th>\n",
       "      <td>16.662244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started month ago</th>\n",
       "      <td>15.329551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>found month ago</th>\n",
       "      <td>14.696983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reason give star</th>\n",
       "      <td>14.659102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spot difference easy</th>\n",
       "      <td>14.158671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wide range topic</th>\n",
       "      <td>13.567425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make feel part</th>\n",
       "      <td>13.067337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guy make laugh</th>\n",
       "      <td>12.121865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>couple year ago</th>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started week ago</th>\n",
       "      <td>11.676355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy medium hard</th>\n",
       "      <td>11.610417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day day life</th>\n",
       "      <td>11.392635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black life matter</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>couple week ago</th>\n",
       "      <td>10.282040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make laugh time</th>\n",
       "      <td>10.072785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long time ago</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "breath fresh air       59.836317\n",
       "long time listener     36.313809\n",
       "make laugh loud        28.050138\n",
       "feel hanging friend    24.250959\n",
       "real estate investing  16.662244\n",
       "started month ago      15.329551\n",
       "found month ago        14.696983\n",
       "reason give star       14.659102\n",
       "spot difference easy   14.158671\n",
       "wide range topic       13.567425\n",
       "make feel part         13.067337\n",
       "guy make laugh         12.121865\n",
       "couple year ago        12.000000\n",
       "started week ago       11.676355\n",
       "easy medium hard       11.610417\n",
       "day day life           11.392635\n",
       "black life matter      11.000000\n",
       "couple week ago        10.282040\n",
       "make laugh time        10.072785\n",
       "long time ago          10.000000"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review TF-IDF: (28249, 1)\n",
      "   true crime\n",
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true crime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true crime\n",
       "0         0.0\n",
       "1         0.0\n",
       "2         0.0\n",
       "3         0.0\n",
       "4         0.0"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "review_vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             min_df=0.01, max_df=0.4, stop_words=\"english\")\n",
    "\n",
    "X_review, review_terms = review_vectorizer.fit_transform(good_reviews.content), review_vectorizer.get_feature_names_out()\n",
    "review_tf_idf = pd.DataFrame(X_review.toarray(), columns=review_terms)\n",
    "print(f\"Review TF-IDF: {review_tf_idf.shape}\")\n",
    "print(review_tf_idf.head(5))\n",
    "review_tf_idf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of X reviews is (28249, 1)\n",
      "Decomposed W reviews matrix is (28249, 3)\n",
      "Decomposed H reviews matrix is (3, 1)\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=3)\n",
    "W_review = nmf.fit_transform(X_review)\n",
    "H_review = nmf.components_\n",
    "print(f\"Original shape of X reviews is {X_review.shape}\")\n",
    "print(f\"Decomposed W reviews matrix is {W_review.shape}\")\n",
    "print(f\"Decomposed H reviews matrix is {H_review.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 2\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "==================================================\n",
      "Reviews Topics:\n",
      "\n",
      "\n",
      "TOPIC 0\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 2\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def get_top_tf_idf_tokens_for_topic(H: np.array, feature_names: List[str], num_top_tokens: int = 5):\n",
    "  \"\"\"\n",
    "  Uses the H matrix (K components x M original features) to identify for each\n",
    "  topic the most frequent tokens.\n",
    "  \"\"\"\n",
    "  for topic, vector in enumerate(H):\n",
    "    print(f\"TOPIC {topic}\\n\")\n",
    "    total = vector.sum()\n",
    "    top_scores = vector.argsort()[::-1][:num_top_tokens]\n",
    "    token_names = list(map(lambda idx: feature_names[idx], top_scores))\n",
    "    strengths = list(map(lambda idx: vector[idx] / total, top_scores))\n",
    "    \n",
    "    for strength, token_name in zip(strengths, token_names):\n",
    "      print(f\"\\b{token_name} ({round(strength * 100, 1)}%)\\n\")\n",
    "    print(f\"=\" * 50)\n",
    "\n",
    "get_top_tf_idf_tokens_for_topic(H_review, review_tf_idf.columns.tolist(), 5)\n",
    "print(f\"Reviews Topics:\\n\\n\")\n",
    "get_top_tf_idf_tokens_for_topic(H_review, review_tf_idf.columns.tolist(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_documents_for_each_topic(W: np.array, documents: List[str], num_docs: int = 5):\n",
    "    sorted_docs = W.argsort(axis=0)[::-1]\n",
    "    top_docs = sorted_docs[:num_docs].T\n",
    "    per_document_totals = W.sum(axis=1)\n",
    "    for topic, top_documents_for_topic in enumerate(top_docs):\n",
    "        print(f\"Topic {topic}\")\n",
    "        for doc in top_documents_for_topic:\n",
    "            score = W[doc][topic]\n",
    "            percent_about_topic = round(score / per_document_totals[doc] * 100, 1)\n",
    "            print(f\"{percent_about_topic}%\", documents[doc])\n",
    "            print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "100.0% hope true crime series . well produced . one annoying .\n",
      "==================================================\n",
      "100.0% wow story , host , detective work journalism . need make true crime list missing .\n",
      "==================================================\n",
      "100.0% true crime fan . also amazing comedy . guy . take two get joke , , ‚Äô hooked . got huge backlog time ! lpotl ! ! !\n",
      "==================================================\n",
      "100.0% started day ago already ! ‚Äô stop ! content , insight , perspective true crime !\n",
      "==================================================\n",
      "100.0% every true crime criticism narrator petty . get . challenge citric put voice know despise hearing voice . annoying review boiled n't someone tell story .\n",
      "==================================================\n",
      "Topic 1\n",
      "0.1% . go plenty detail case ; ‚Äô heard several case , gained much clearer understanding tell . ‚Äô objective fact-based . biggest issue lot true crime sensationalization speculation ; lay fact objective way . ‚Äô recommend enough ! ! !\n",
      "==================================================\n",
      "0.1% david ‚Äô speaking story style . nice switch popular seem similar candor . come across caring empathetic think important . american , interesting hear canadian true crime .\n",
      "==================================================\n",
      "0.1% true crime right . jasmyne - troy , ny\n",
      "==================================================\n",
      "0.1% awesome true crime ever made .\n",
      "==================================================\n",
      "0.1% season one got involved true crime . season ‚Äô hope season better . far ‚Äô pretty .\n",
      "==================================================\n",
      "Topic 2\n",
      "0.1% ‚Äô always looking different true crime way find ! excited start !\n",
      "==================================================\n",
      "0.1% update : getting little bored new ! ! every true crime one far favorite . probably help ‚Äô year old female , way talk relatable . ‚Äô since beginning almost feel protective guy read bad review ü§£ . think people take way someone talk seriously , ‚Äô . tell , girl research . care victim , see review someone said spoke bad deceased victim shoe ? ........ seriously ? intended . lot research , sometimes speaking victim family , advocating people ‚Äô advocate . could ask ? keep work girl üëèüèºüëèüèºüëèüèº\n",
      "==================================================\n",
      "0.1% ! ! ! recommended crime junky . binging week , one .... downloaded . think fun way learn true crime , documentary may found . palette cleanser end , music always fun upbeat , end dancing . one critique would start assuming know story crime , sometimes little lost get . maybe little intro ‚Äô go amiss . new listener ... start beginning ! ! many throw back past , ‚Äô loop ! ! also , need cup say : black beauty scotch , go sit , would ! !\n",
      "==================================================\n",
      "0.1% exactly meant . ‚Äô done well , however ‚Äô faint heart . true crime thick skinned & detail may . ‚Äú sword & scale ‚Äù üòâüòâ ‚Äô missed . willing disturbed give . sensitive & probably pa .\n",
      "==================================================\n",
      "0.1% ali charlie make listener feel right studio part conversation ! accomplishment considering host n't even studio ! thank diligent research case , earth attitude , dedication making outstanding . eagerly await weekly new . sometimes 'll even re- format style dynamic charlie ali create . would recommend anyone enjoys mystery true crime story . thanks guy , producing awesomely unique enjoyable u listener fan . much , elyssa miami .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "get_top_documents_for_each_topic(W_review, good_reviews.content.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Justin/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value ['ner', 'parser'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# load the language model, but we disable the ner (named entity recognition) and parser (dependency parser)\n",
    "# since we don't need them for our use case to speed things up\n",
    "nlp = spacy.load('en_core_web_md', disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def process_text(text):\n",
    "  \"\"\"\n",
    "  This function will use Spacy to perform stopword removal and lemmatization.\n",
    "  \"\"\"\n",
    "  doc = nlp(text)\n",
    "  processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "  # this will get the word2vec embeddings for the processed text (the average of each token in the doc's word2vec embeddings)\n",
    "  return np.array(nlp(processed_text).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-110-cc89d5e0bf97>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good_reviews[\"vectors\"] = good_reviews.content.apply(process_text)\n"
     ]
    }
   ],
   "source": [
    "# use pandas' apply(...) method to apply this process_text function to each row's text field\n",
    "good_reviews[\"vectors\"] = good_reviews.content.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.005518933, -0.5161569, -0.559347, -0.536282...\n",
       "2        [-0.51205826, -0.5742024, -1.2540163, 0.385765...\n",
       "4        [1.2810528, -0.66930777, -0.19595638, -2.56741...\n",
       "5        [0.0016238403, -0.28106058, -0.54703975, -1.69...\n",
       "7        [0.8212379, -0.005584643, -1.3926145, -1.49905...\n",
       "                               ...                        \n",
       "49992    [0.70252043, -0.18376486, -0.09802313, -1.5482...\n",
       "49993    [5.0891, -3.3753002, -4.2695, -4.8156, 3.8904,...\n",
       "49994    [1.1265317, -0.6807913, -2.5832686, -0.7725373...\n",
       "49996    [-0.56076795, -0.5616698, -0.32426855, -0.1006...\n",
       "49999    [1.4425428, -1.4996988, -2.84881, -1.6160922, ...\n",
       "Name: vectors, Length: 28249, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have for each document a numpy array of 300 numbers (its word2vec embeddings)\n",
    "good_reviews[\"vectors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_clean(review):\n",
    "    review = review.strip().lower()\n",
    "    review = re.sub(r'\\'ve|\\'s|\\'m|\\'re|\\d', '', review)\n",
    "    review = re.sub(r'\\b(pod|podcast|great|really|good|listen|episode|highly)+(s?|es?)\\b','', review)\n",
    "    review = re.sub(r'\\b(listen|listens|listening|listened|forward|look forward|looking forward|loos forward)\\b', '', review)\n",
    "    review = re.sub(r'\\b(love|like|best|hate|worst|bad)+(s?|es?)\\b','', review)\n",
    "\n",
    "    \n",
    "    word_tokens = word_tokenize(review)\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    word_tokens = filtered_sentence\n",
    "#     word_tokens = [pt.stem(w) for w in word_tokens]\n",
    "    word_tokens = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    text_clean = ' '.join(word_tokens)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-127-58230dc2584c>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_reviews['content'] = bad_reviews.apply(lambda x: review_clean(x['content']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "bad_reviews['content'] = bad_reviews.apply(lambda x: review_clean(x['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 2403),\n",
       " ('people', 2002),\n",
       " ('host', 1819),\n",
       " ('make', 1712),\n",
       " ('story', 1619),\n",
       " ('way', 1406),\n",
       " ('guy', 1389),\n",
       " ('know', 1366),\n",
       " ('think', 1313),\n",
       " ('want', 1311),\n",
       " ('talk', 1280),\n",
       " ('thing', 1218),\n",
       " ('say', 1187),\n",
       " ('better', 1158),\n",
       " ('used', 1106),\n",
       " ('need', 1098),\n",
       " ('sound', 1003),\n",
       " ('year', 986),\n",
       " ('feel', 960),\n",
       " ('hear', 940),\n",
       " ('content', 936),\n",
       " ('minute', 916),\n",
       " ('guest', 899),\n",
       " ('talking', 885),\n",
       " ('voice', 821),\n",
       " ('new', 815),\n",
       " ('hard', 812),\n",
       " ('come', 809),\n",
       " ('lot', 799),\n",
       " ('going', 789),\n",
       " ('stop', 783),\n",
       " ('interesting', 763),\n",
       " ('opinion', 747),\n",
       " ('annoying', 743),\n",
       " ('topic', 736),\n",
       " ('review', 734),\n",
       " ('fact', 723),\n",
       " ('point', 721),\n",
       " ('trying', 710),\n",
       " ('listener', 709),\n",
       " ('woman', 681),\n",
       " ('actually', 675),\n",
       " ('try', 658),\n",
       " ('life', 647),\n",
       " ('right', 643),\n",
       " ('fan', 638),\n",
       " ('thought', 626),\n",
       " ('work', 624),\n",
       " ('star', 612),\n",
       " ('long', 612)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(stop_words=\"english\", binary=True).fit(bad_reviews[\"content\"])\n",
    "bag_of_words = vec.transform(bad_reviews[\"content\"])\n",
    "sum_words = bag_of_words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "words_freq[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=0.4, max_features=200, stop_words=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(bad_reviews[\"content\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Justin/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:404: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aldiz', 'baiknya', 'baizik', 'berkali', 'bukatzeko', 'edota', 'eze', 'ezpabere', 'ezpada', 'ezperen', 'gainera', 'gainerontzean', 'guztiz', 'hainbestez', 'horra', 'kali', 'kurangnya', 'mata', 'olah', 'ordea', 'osterantzean', 'printr', 'sekurang', 'setidak', 'tama', 'tidaknya'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/Justin/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual fact case</th>\n",
       "      <th>add insult injury</th>\n",
       "      <th>amount time word</th>\n",
       "      <th>angry loud spoiled</th>\n",
       "      <th>annoying vocal fry</th>\n",
       "      <th>anti law enforcement</th>\n",
       "      <th>asks question guest</th>\n",
       "      <th>audio quality horrible</th>\n",
       "      <th>audio quality poor</th>\n",
       "      <th>audio quality terrible</th>\n",
       "      <th>...</th>\n",
       "      <th>waste time woman</th>\n",
       "      <th>watch youtube video</th>\n",
       "      <th>wing talking point</th>\n",
       "      <th>wish give star</th>\n",
       "      <th>wish give zero</th>\n",
       "      <th>wolf sheep clothing</th>\n",
       "      <th>woman spend time</th>\n",
       "      <th>writing negative review</th>\n",
       "      <th>year ago started</th>\n",
       "      <th>year high school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16447</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16448</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16449</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16450</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16451</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16452 rows √ó 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actual fact case  add insult injury  amount time word  \\\n",
       "0                   0.0                0.0               0.0   \n",
       "1                   0.0                0.0               0.0   \n",
       "2                   0.0                0.0               0.0   \n",
       "3                   0.0                0.0               0.0   \n",
       "4                   0.0                0.0               0.0   \n",
       "...                 ...                ...               ...   \n",
       "16447               0.0                0.0               0.0   \n",
       "16448               0.0                0.0               0.0   \n",
       "16449               0.0                0.0               0.0   \n",
       "16450               0.0                0.0               0.0   \n",
       "16451               0.0                0.0               0.0   \n",
       "\n",
       "       angry loud spoiled  annoying vocal fry  anti law enforcement  \\\n",
       "0                     0.0                 0.0                   0.0   \n",
       "1                     0.0                 0.0                   0.0   \n",
       "2                     0.0                 0.0                   0.0   \n",
       "3                     0.0                 0.0                   0.0   \n",
       "4                     0.0                 0.0                   0.0   \n",
       "...                   ...                 ...                   ...   \n",
       "16447                 0.0                 0.0                   0.0   \n",
       "16448                 0.0                 0.0                   0.0   \n",
       "16449                 0.0                 0.0                   0.0   \n",
       "16450                 0.0                 0.0                   0.0   \n",
       "16451                 0.0                 0.0                   0.0   \n",
       "\n",
       "       asks question guest  audio quality horrible  audio quality poor  \\\n",
       "0                      0.0                     0.0                 0.0   \n",
       "1                      0.0                     0.0                 0.0   \n",
       "2                      0.0                     0.0                 0.0   \n",
       "3                      0.0                     0.0                 0.0   \n",
       "4                      0.0                     0.0                 0.0   \n",
       "...                    ...                     ...                 ...   \n",
       "16447                  0.0                     0.0                 0.0   \n",
       "16448                  0.0                     0.0                 0.0   \n",
       "16449                  0.0                     0.0                 0.0   \n",
       "16450                  0.0                     0.0                 0.0   \n",
       "16451                  0.0                     0.0                 0.0   \n",
       "\n",
       "       audio quality terrible  ...  waste time woman  watch youtube video  \\\n",
       "0                         0.0  ...               0.0                  0.0   \n",
       "1                         0.0  ...               0.0                  0.0   \n",
       "2                         0.0  ...               0.0                  0.0   \n",
       "3                         0.0  ...               0.0                  0.0   \n",
       "4                         0.0  ...               0.0                  0.0   \n",
       "...                       ...  ...               ...                  ...   \n",
       "16447                     0.0  ...               0.0                  0.0   \n",
       "16448                     0.0  ...               0.0                  0.0   \n",
       "16449                     0.0  ...               0.0                  0.0   \n",
       "16450                     0.0  ...               0.0                  0.0   \n",
       "16451                     0.0  ...               0.0                  0.0   \n",
       "\n",
       "       wing talking point  wish give star  wish give zero  \\\n",
       "0                     0.0             0.0             0.0   \n",
       "1                     0.0             0.0             0.0   \n",
       "2                     0.0             0.0             0.0   \n",
       "3                     0.0             0.0             0.0   \n",
       "4                     0.0             0.0             0.0   \n",
       "...                   ...             ...             ...   \n",
       "16447                 0.0             0.0             0.0   \n",
       "16448                 0.0             0.0             0.0   \n",
       "16449                 0.0             0.0             0.0   \n",
       "16450                 0.0             0.0             0.0   \n",
       "16451                 0.0             0.0             0.0   \n",
       "\n",
       "       wolf sheep clothing  woman spend time  writing negative review  \\\n",
       "0                      0.0               0.0                      0.0   \n",
       "1                      0.0               0.0                      0.0   \n",
       "2                      0.0               0.0                      0.0   \n",
       "3                      0.0               0.0                      0.0   \n",
       "4                      0.0               0.0                      0.0   \n",
       "...                    ...               ...                      ...   \n",
       "16447                  0.0               0.0                      0.0   \n",
       "16448                  0.0               0.0                      0.0   \n",
       "16449                  0.0               0.0                      0.0   \n",
       "16450                  0.0               0.0                      0.0   \n",
       "16451                  0.0               0.0                      0.0   \n",
       "\n",
       "       year ago started  year high school  \n",
       "0                   0.0               0.0  \n",
       "1                   0.0               0.0  \n",
       "2                   0.0               0.0  \n",
       "3                   0.0               0.0  \n",
       "4                   0.0               0.0  \n",
       "...                 ...               ...  \n",
       "16447               0.0               0.0  \n",
       "16448               0.0               0.0  \n",
       "16449               0.0               0.0  \n",
       "16450               0.0               0.0  \n",
       "16451               0.0               0.0  \n",
       "\n",
       "[16452 rows x 200 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>long time listener</th>\n",
       "      <td>44.349002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blah blah blah</th>\n",
       "      <td>18.515546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give zero star</th>\n",
       "      <td>16.691483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spend time talking</th>\n",
       "      <td>13.985865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>black life matter</th>\n",
       "      <td>13.915095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full body chill</th>\n",
       "      <td>13.679891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total waste time</th>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voice nail chalkboard</th>\n",
       "      <td>10.677957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social justice warrior</th>\n",
       "      <td>8.678575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop interrupting guest</th>\n",
       "      <td>8.678575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give benefit doubt</th>\n",
       "      <td>8.157076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mental health issue</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor audio quality</th>\n",
       "      <td>7.743129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high school girl</th>\n",
       "      <td>7.062994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish give star</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minute life back</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste time host</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio quality terrible</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold case file</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>audio quality poor</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             score\n",
       "long time listener       44.349002\n",
       "blah blah blah           18.515546\n",
       "give zero star           16.691483\n",
       "spend time talking       13.985865\n",
       "black life matter        13.915095\n",
       "full body chill          13.679891\n",
       "total waste time         11.000000\n",
       "voice nail chalkboard    10.677957\n",
       "social justice warrior    8.678575\n",
       "stop interrupting guest   8.678575\n",
       "give benefit doubt        8.157076\n",
       "mental health issue       8.000000\n",
       "poor audio quality        7.743129\n",
       "high school girl          7.062994\n",
       "wish give star            7.000000\n",
       "minute life back          7.000000\n",
       "waste time host           7.000000\n",
       "audio quality terrible    7.000000\n",
       "cold case file            7.000000\n",
       "audio quality poor        7.000000"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review TF-IDF: (16452, 3)\n",
      "   true crime  want hear  waste time\n",
      "0         0.0        0.0         0.0\n",
      "1         0.0        0.0         0.0\n",
      "2         0.0        0.0         0.0\n",
      "3         0.0        0.0         0.0\n",
      "4         0.0        0.0         0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true crime</th>\n",
       "      <th>want hear</th>\n",
       "      <th>waste time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true crime  want hear  waste time\n",
       "0         0.0        0.0         0.0\n",
       "1         0.0        0.0         0.0\n",
       "2         0.0        0.0         0.0\n",
       "3         0.0        0.0         0.0\n",
       "4         0.0        0.0         0.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "review_vectorizer = TfidfVectorizer(ngram_range=(2,2),\n",
    "                             min_df=0.01, max_df=0.4, stop_words=\"english\")\n",
    "\n",
    "X_review, review_terms = review_vectorizer.fit_transform(bad_reviews.content), review_vectorizer.get_feature_names_out()\n",
    "review_tf_idf = pd.DataFrame(X_review.toarray(), columns=review_terms)\n",
    "print(f\"Review TF-IDF: {review_tf_idf.shape}\")\n",
    "print(review_tf_idf.head(5))\n",
    "review_tf_idf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of X reviews is (16452, 3)\n",
      "Decomposed W reviews matrix is (16452, 3)\n",
      "Decomposed H reviews matrix is (3, 3)\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=3)\n",
    "W_review = nmf.fit_transform(X_review)\n",
    "H_review = nmf.components_\n",
    "print(f\"Original shape of X reviews is {X_review.shape}\")\n",
    "print(f\"Decomposed W reviews matrix is {W_review.shape}\")\n",
    "print(f\"Decomposed H reviews matrix is {H_review.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0\n",
      "\n",
      "\bwaste time (100.0%)\n",
      "\n",
      "\btrue crime (0.0%)\n",
      "\n",
      "\bwant hear (0.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "\bwant hear (0.0%)\n",
      "\n",
      "\bwaste time (0.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 2\n",
      "\n",
      "\bwant hear (100.0%)\n",
      "\n",
      "\bwaste time (0.0%)\n",
      "\n",
      "\btrue crime (0.0%)\n",
      "\n",
      "==================================================\n",
      "Reviews Topics:\n",
      "\n",
      "\n",
      "TOPIC 0\n",
      "\n",
      "\bwaste time (100.0%)\n",
      "\n",
      "\btrue crime (0.0%)\n",
      "\n",
      "\bwant hear (0.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\btrue crime (100.0%)\n",
      "\n",
      "\bwant hear (0.0%)\n",
      "\n",
      "\bwaste time (0.0%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 2\n",
      "\n",
      "\bwant hear (100.0%)\n",
      "\n",
      "\bwaste time (0.0%)\n",
      "\n",
      "\btrue crime (0.0%)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "def get_top_tf_idf_tokens_for_topic(H: np.array, feature_names: List[str], num_top_tokens: int = 5):\n",
    "  \"\"\"\n",
    "  Uses the H matrix (K components x M original features) to identify for each\n",
    "  topic the most frequent tokens.\n",
    "  \"\"\"\n",
    "  for topic, vector in enumerate(H):\n",
    "    print(f\"TOPIC {topic}\\n\")\n",
    "    total = vector.sum()\n",
    "    top_scores = vector.argsort()[::-1][:num_top_tokens]\n",
    "    token_names = list(map(lambda idx: feature_names[idx], top_scores))\n",
    "    strengths = list(map(lambda idx: vector[idx] / total, top_scores))\n",
    "    \n",
    "    for strength, token_name in zip(strengths, token_names):\n",
    "      print(f\"\\b{token_name} ({round(strength * 100, 1)}%)\\n\")\n",
    "    print(f\"=\" * 50)\n",
    "\n",
    "get_top_tf_idf_tokens_for_topic(H_review, review_tf_idf.columns.tolist(), 5)\n",
    "print(f\"Reviews Topics:\\n\\n\")\n",
    "get_top_tf_idf_tokens_for_topic(H_review, review_tf_idf.columns.tolist(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_documents_for_each_topic(W: np.array, documents: List[str], num_docs: int = 5):\n",
    "    sorted_docs = W.argsort(axis=0)[::-1]\n",
    "    top_docs = sorted_docs[:num_docs].T\n",
    "    per_document_totals = W.sum(axis=1)\n",
    "    for topic, top_documents_for_topic in enumerate(top_docs):\n",
    "        print(f\"Topic {topic}\")\n",
    "        for doc in top_documents_for_topic:\n",
    "            score = W[doc][topic]\n",
    "            percent_about_topic = round(score / per_document_totals[doc] * 100, 1)\n",
    "            print(f\"{percent_about_topic}%\", documents[doc])\n",
    "            print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "100.0% ‚Äô waste time prideful person\n",
      "==================================================\n",
      "100.0% perhaps ‚Äô ever . host juvenile annoying . shockingly , seem knowledge artist discussing popular music history . terrible waste time .\n",
      "==================================================\n",
      "100.0% show started strong get past tell stretching could accomplished minute purpose ad space . random hearsay completely pointless recording junior detective waste time . get substance !\n",
      "==================================================\n",
      "100.0% sadly mostly ad boring host story ... ghost story submitted caller . also waste time host read partial story , hoping people pay hear rest . disappointing !\n",
      "==================================================\n",
      "100.0% sorry , ca n't take `` woe `` anymore . five year , nothing bunch navel gazing , narrow minded child-adults . growth , le le entertainment . sorry guy , waste time .\n",
      "==================================================\n",
      "Topic 1\n",
      "100.0% first , started pretty . mostly looking true crime fan . however , could longer stomach political propaganda took leave . wanted well done true crime show , another political commentary . disappointed .\n",
      "==================================================\n",
      "100.0% first started simply true crime . get hear informative narrative get hear accent . southern girl always fascinated people different states/countries different dialect .\n",
      "==================================================\n",
      "100.0% place . blatantly making fun murder victim bringing politics , ‚Äô get . thing true crime badly wanted .\n",
      "==================================================\n",
      "100.0% omg voice , ‚Äô know one picked awful . extremely robotic , almost voice call get automated message . feel true crime , serious topic , narrated someone deep voice . someone sound passion behind ; full wisdom . content ‚Äô seem , ‚Äô continue voice god awful . get new narrator , guarantee would listener .\n",
      "==================================================\n",
      "100.0% updated review previously reviewed ( ) feel need amend . unsubscribe repetitive nature content . feel dozen fostering , adoption , eating disorder , dieting , , amy ‚Äô business . think amy lovely personality attitude towards life , would see broaden horizon . also felt uncomfortable recent disclosed mother ‚Äô abortion . sharing someone ‚Äô medical history , even deceased , okay , way topic discussed subjective . ‚Äî original review first , show . ‚Äô different , mainly pop culture , investigative story true crime . amy realness lightness lends well positive , upbeat , relatable . show life feeling heavy dark need encouragement idea small lifestyle change make big difference . would encourage amy mindful guest , way presented message share . recent , bethany ugarte share experience ibs breast implant . bethany ‚Äô story interesting certainly worth sharing , think ‚Äô important listener know bethany health professional . spoke opinion experience fact , misleading . example , would appreciated amy using bethanys story introduction breast implant topic speaking health professional confirm whether bethany ‚Äô statement breast implant causing health issue accurate . hearing alternative perspective , think ‚Äô important verify information sharing . additionally , amy ‚Äô sister recently share ‚Äú rule dating daughter . ‚Äù segment meant lighthearted funny , ‚Äú rule ‚Äù , amy admitted , dated . think ‚Äô important talk kid teenager dating ‚Äô glad amy started conversation , would liked hear healthier spin ‚Äú rule . ‚Äù cringed christy read felt degrading hypothetical daughter boy taking . boy slimy rule make , perhaps christy ‚Äô daughter share rule dating . overall , think sharing quality healthy relationship , warning sign red flag look , tip parent kid dating would refreshing way cover topic . even concern , keep amy presence far guest enjoy ‚Äô .\n",
      "==================================================\n",
      "Topic 2\n",
      "100.0% quite simply multi-sport . almost unlistenable . care ‚Äú coffee club ‚Äù winner story . interview absolutely nothing either entertain educate . feel free announce winner maybe even give short update race went , getting ridiculous . went never miss , nothing else available . know harsh review would want hear truth .\n",
      "==================================================\n",
      "100.0% kleeper everything , governor kasich , consider moderate republican , kept talking ‚Äú bringing thing back ‚Äù julie bowen . whether family social norm , desire yesteryear ‚Äô want hear . sound older white guy apart gop ‚Äô want progress accept fact progressing society without .\n",
      "==================================================\n",
      "100.0% used - motivating , tangible , easy . since re-brand , connect . ‚Äô people evolving , ‚Äô genuinely confused‚Ä¶ ‚Äô poorly edited youtube/conference copy paste ? painfully apparent team slap without hearing first . secondly , probably annoyed feedback - many celebrity interview ? ‚Äô want hear celebs need project promotion covid stay relevant , want hear rachel . conversation forced e ! interview ‚Äô even aware rachel brand interview ‚Äô uncomfortable ! took time day hope rach team use constructive feedback .\n",
      "==================================================\n",
      "100.0% joe , jamie need hit cattle prod go rant . quickly ‚Äú long form conversation ‚Äù devolve rambling mike tyson something else . passion want hear guest !\n",
      "==================================================\n",
      "100.0% ‚Äô wish ‚Äô talk people much , want hear celebrity scientist monica ‚Äô cheer life high school ‚Äô talking global scientist constantly saying ‚Äú yea ‚Äù ‚Äú right ‚Äù ‚Äô annoying interview .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "get_top_documents_for_each_topic(W_review, bad_reviews.content.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Justin/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value ['ner', 'parser'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# load the language model, but we disable the ner (named entity recognition) and parser (dependency parser)\n",
    "# since we don't need them for our use case to speed things up\n",
    "nlp = spacy.load('en_core_web_md', disable = ['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def process_text(text):\n",
    "  \"\"\"\n",
    "  This function will use Spacy to perform stopword removal and lemmatization.\n",
    "  \"\"\"\n",
    "  doc = nlp(text)\n",
    "  processed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "  # this will get the word2vec embeddings for the processed text (the average of each token in the doc's word2vec embeddings)\n",
    "  return np.array(nlp(processed_text).vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-142-7dd07aa31ad1>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad_reviews[\"vectors\"] = bad_reviews.content.apply(process_text)\n"
     ]
    }
   ],
   "source": [
    "# use pandas' apply(...) method to apply this process_text function to each row's text field\n",
    "bad_reviews[\"vectors\"] = bad_reviews.content.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        [0.32942572, 0.998488, -0.54330236, 0.79712933...\n",
       "6        [-0.86089176, -0.37215918, -1.6511924, -1.6320...\n",
       "8        [1.7946743, 2.6819084, -0.10883585, -0.5083608...\n",
       "10       [0.40497166, -0.05002085, -1.576666, -0.614707...\n",
       "14       [-0.02933349, 0.56123495, -1.7128899, -1.30029...\n",
       "                               ...                        \n",
       "49987    [-0.1701629, -0.48754764, -1.2045877, -1.00413...\n",
       "49988    [-1.3511939, -0.9265924, 1.070314, -0.48081094...\n",
       "49995    [-1.7057341, -3.6644633, -7.7446094, 0.7407636...\n",
       "49997    [-1.015794, -0.70189905, -0.41488737, -0.63473...\n",
       "49998    [-1.1142291, -1.3235112, -3.483763, -1.5118328...\n",
       "Name: vectors, Length: 16452, dtype: object"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have for each document a numpy array of 300 numbers (its word2vec embeddings)\n",
    "bad_reviews[\"vectors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
